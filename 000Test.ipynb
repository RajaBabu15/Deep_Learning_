{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Label of training set has the shape of  : (60000, 28, 28)\n",
      "Output Label of training set has the shape of : (60000, 10)\n",
      "Input Label of test set has the shape of      : (10000, 28, 28)\n",
      "Output Label of test set has the shape of     : (10000, 10)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 64)        640       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5770      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,266\n",
      "Trainable params: 80,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 24s 51ms/step - loss: 0.2640 - accuracy: 0.9187\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 24s 50ms/step - loss: 0.0681 - accuracy: 0.9787\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 23s 50ms/step - loss: 0.0483 - accuracy: 0.9849\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 25s 53ms/step - loss: 0.0395 - accuracy: 0.9878\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 26s 55ms/step - loss: 0.0319 - accuracy: 0.9899\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 25s 53ms/step - loss: 0.0287 - accuracy: 0.9911\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 24s 51ms/step - loss: 0.0248 - accuracy: 0.9924\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 25s 52ms/step - loss: 0.0223 - accuracy: 0.9926\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 24s 51ms/step - loss: 0.0189 - accuracy: 0.9941\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 24s 52ms/step - loss: 0.0175 - accuracy: 0.9940\n",
      "\n",
      "Test accuracy: 99.3%\n"
     ]
    }
   ],
   "source": [
    "''' CNN MNIST digits classification\n",
    "\n",
    "3-layer CNN for MNIST digits classification \n",
    "First 2 layers - Conv2D-ReLU-MaxPool\n",
    "3rd layer - Conv2D-ReLU-Dropout\n",
    "4th layer - Dense(10)\n",
    "Output Activation - softmax\n",
    "Optimizer - Adam\n",
    "\n",
    "99.4% test accuracy in 10epochs\n",
    "\n",
    "https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# load mnist dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# compute the number of labels\n",
    "num_labels = len(np.unique(y_train))\n",
    "\n",
    "# convert to one-hot vector\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# printing the shape of the training ans test set\n",
    "print(f\"Input Label of training set has the shape of  : {x_train.shape}\")\n",
    "print(f\"Output Label of training set has the shape of : {y_train.shape}\")\n",
    "print(f\"Input Label of test set has the shape of      : {x_test.shape}\")\n",
    "print(f\"Output Label of test set has the shape of     : {y_test.shape}\")\n",
    "\n",
    "# input image dimensions\n",
    "image_size = x_train.shape[1]\n",
    "# resize and normalize\n",
    "x_train = np.reshape(x_train,[-1, image_size, image_size, 1])\n",
    "x_test = np.reshape(x_test,[-1, image_size, image_size, 1])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# network parameters\n",
    "# image is processed as is (square grayscale)\n",
    "input_shape = (image_size, image_size, 1)\n",
    "batch_size = 128\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "filters = 64\n",
    "dropout = 0.2\n",
    "\n",
    "# model is a stack of CNN-ReLU-MaxPooling\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=filters,\n",
    "                 kernel_size=kernel_size,\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size))\n",
    "model.add(Conv2D(filters=filters,\n",
    "                 kernel_size=kernel_size,\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size))\n",
    "model.add(Conv2D(filters=filters,\n",
    "                 kernel_size=kernel_size,\n",
    "                 activation='relu'))\n",
    "model.add(Flatten())\n",
    "# dropout added as regularizer\n",
    "model.add(Dropout(dropout))\n",
    "# output layer is 10-dim one-hot vector\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# enable this if pydot can be installed\n",
    "# pip install pydot\n",
    "plot_model(model, to_file='cnn-mnist.png', show_shapes=True)\n",
    "\n",
    "# loss function for one-hot vector\n",
    "# use of adam optimizer\n",
    "# accuracy is good metric for classification tasks\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# train the network\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=batch_size)\n",
    "\n",
    "_, acc = model.evaluate(x_test,\n",
    "                        y_test,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\raja babu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.20.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_241 (Dense)           (None, 8)                 16        \n",
      "                                                                 \n",
      " dense_242 (Dense)           (None, 16)                144       \n",
      "                                                                 \n",
      " dense_243 (Dense)           (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 177\n",
      "Trainable params: 177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3/3 [==============================] - 1s 2ms/step - loss: 9.8026\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 9.6416\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 9.4464\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 9.3034\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 9.1189\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 8.9655\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 8.7853\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 8.6371\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 999us/step - loss: 8.4542\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 8.2826\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8.1141\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.9666\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 7.7943\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 7.6219\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.4402\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.2706\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 7.0871\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 6.9191\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 6.7354\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 6.5382\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 6.3437\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 6.1630\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 5.9772\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 5.7620\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 5.5693\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 5.3537\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 5.1500\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 4.9465\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 4.7086\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 4.5083\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 4.2888\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 4.0723\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 3.8493\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 3.6284\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 3.4201\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3.2150\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 3.0084\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.7770\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.5816\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.3816\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.1889\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.9977\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.8089\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6409\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4743\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.3099\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1664\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0241\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8907\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7768\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6647\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5672\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4745\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3954\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3272\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2676\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2144\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1720\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1355\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1060\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0833\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0645\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0496\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0382\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0309\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0248\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0206\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0174\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0154\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0145\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0134\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0129\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0127\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0124\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0125\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0124\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0124\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0123\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0123\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0124\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0123\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0123\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0123\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0122\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0122\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0123\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0123\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0123\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0123\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0123\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0123\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0123\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0124\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0123\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0123\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0123\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0123\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0123\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0123\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0123\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "k (Linear Algebra Method):\n",
      "[[2.00052158]\n",
      " [2.95845279]]\n",
      "Ground Truth, Linear Alg Prediction, MLP Prediction\n",
      "[[1.         1.10765681 1.1108973 ]\n",
      " [1.2        1.33326044 1.3358885 ]\n",
      " [1.4        1.44274306 1.44507384]\n",
      " [1.6        1.65845509 1.6602006 ]\n",
      " [1.8        1.68544018 1.68711221]\n",
      " [2.         2.0617941  2.06244445]\n",
      " [2.2        2.10617837 2.10670829]\n",
      " [2.4        2.45123818 2.45083117]\n",
      " [2.6        2.46427186 2.46382976]\n",
      " [2.8        2.68575377 2.6847105 ]\n",
      " [3.         3.04223136 3.04022026]\n",
      " [3.2        3.16217659 3.15983987]\n",
      " [3.4        3.23489564 3.23236156]\n",
      " [3.6        3.74391137 3.73999596]\n",
      " [3.8        3.60090393 3.59737659]\n",
      " [4.         4.14810548 4.14309311]\n",
      " [4.2        4.05662633 4.05186224]\n",
      " [4.4        4.50773309 4.50174475]\n",
      " [4.6        4.6457565  4.63939333]\n",
      " [4.8        4.86086783 4.85392094]]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "2*2+3=7,pred= 6.946852 diff= -0.05314779281616211\n",
      "m=2,c=3, predicted m= 1.9950914 ,c= 2.956669\n"
     ]
    }
   ],
   "source": [
    "'''A simple MLP in Keras implementing linear regression.\n",
    "'''\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# numpy package\n",
    "import numpy as np\n",
    "\n",
    "# keras modules\n",
    "from  keras.models import Sequential\n",
    "from  keras.layers import Dense \n",
    "from  keras.utils import plot_model\n",
    "\n",
    "# generate x data\n",
    "x = np.arange(-1,1,0.1)\n",
    "x = np.reshape(x, [-1,1])\n",
    "\n",
    "# generate y data\n",
    "y = 2 * x + 3\n",
    "\n",
    "# True if noise is added to y\n",
    "is_noisy = True\n",
    "\n",
    "# add noise if enabled\n",
    "if is_noisy:\n",
    "    noise = np.random.uniform(-0.1, 0.1, x.shape)\n",
    "    x = x + noise\n",
    "\n",
    "# deep learning method\n",
    "# build 2-layer MLP network \n",
    "model = Sequential()\n",
    "# 1st MLP has 8 units (perceptron), input is 1-dim\n",
    "model.add(Dense(units=8, input_dim=1))\n",
    "# 2nd MLP has 1 unit, output is 1-dim\n",
    "model.add(Dense(units=16))\n",
    "model.add(Dense(units=1))\n",
    "# print summary to double check the network\n",
    "model.summary()\n",
    "# create a nice image of the network model\n",
    "# enable this if pydot can be installed\n",
    "# pip install pydot\n",
    "#plot_model(model, to_file='linear-model.png', show_shapes=True)\n",
    "# indicate the loss function and use stochastic gradient descent\n",
    "# (sgd) as optimizer\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "# feed the network with complete dataset (1 epoch) 100 times\n",
    "# batch size of sgd is 4\n",
    "model.fit(x, y, epochs=100, batch_size=8)\n",
    "# simple validation by predicting the output based on x\n",
    "ypred = model.predict(x)\n",
    "\n",
    "# linear algebra method\n",
    "ones = np.ones(x.shape)\n",
    "# A is the concat of x and 1s\n",
    "A = np.concatenate([x,ones], axis=1)\n",
    "# compute k using using pseudo-inverse\n",
    "k = np.matmul(np.linalg.pinv(A), y) \n",
    "print(\"k (Linear Algebra Method):\")\n",
    "print(k)\n",
    "# predict the output using linear algebra solution\n",
    "yla = np.matmul(A, k)\n",
    "\n",
    "# print ground truth, linear algebra, MLP solutions\n",
    "outputs = np.concatenate([y, yla, ypred], axis=1)\n",
    "print(\"Ground Truth, Linear Alg Prediction, MLP Prediction\")\n",
    "print(outputs)\n",
    "\n",
    "# Uncomment to see the output for a new input data \n",
    "# that is not part of the training data.\n",
    "x = np.array([2,1,0])\n",
    "ypred = model.predict(x)\n",
    "print(\"2*2+3=7,pred=\",ypred[0][0],\"diff=\",ypred[0][0]-7)\n",
    "print(\"m=2,c=3,\",\"predicted m=\",str(ypred[0][0]-ypred[1][0]),\",c=\",ypred[2][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 256)               200960    \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4283 - accuracy: 0.8688\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1966 - accuracy: 0.9410\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1507 - accuracy: 0.9545\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1269 - accuracy: 0.9616\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1130 - accuracy: 0.9652\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1006 - accuracy: 0.9690\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0948 - accuracy: 0.9704\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0879 - accuracy: 0.9722\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0819 - accuracy: 0.9750\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0783 - accuracy: 0.9752\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0718 - accuracy: 0.9770\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0688 - accuracy: 0.9786\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0683 - accuracy: 0.9781\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0653 - accuracy: 0.9790\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0592 - accuracy: 0.9813\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0586 - accuracy: 0.9816\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0591 - accuracy: 0.9811\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0553 - accuracy: 0.9822\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0535 - accuracy: 0.9826\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0539 - accuracy: 0.9828\n",
      "\n",
      "Test accuracy: 98.2%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A MLP network for MNIST digits classification\n",
    "\n",
    "98.3% test accuracy in 20epochs\n",
    "\n",
    "https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras\n",
    "'''\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from  keras.models import Sequential\n",
    "from  keras.layers import Dense, Activation, Dropout\n",
    "from  keras.utils import to_categorical, plot_model\n",
    "from  keras.datasets import mnist\n",
    "\n",
    "# load mnist dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# compute the number of labels\n",
    "num_labels = len(np.unique(y_train))\n",
    "\n",
    "# convert to one-hot vector\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# image dimensions (assumed square)\n",
    "image_size = x_train.shape[1]\n",
    "input_size = image_size * image_size\n",
    "\n",
    "# resize and normalize\n",
    "x_train = np.reshape(x_train, [-1, input_size])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = np.reshape(x_test, [-1, input_size])\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# network parameters\n",
    "batch_size = 128\n",
    "hidden_units = 256\n",
    "dropout = 0.45\n",
    "\n",
    "# model is a 3-layer MLP with ReLU and dropout after each layer\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_units, input_dim=input_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(hidden_units))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(num_labels))\n",
    "# this is the output for one-hot vector\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "#\n",
    "# enable this if pydot can be installed\n",
    "# pip install pydot\n",
    "#plot_model(model, to_file='mlp-mnist.png', show_shapes=True)\n",
    "\n",
    "# loss function for one-hot vector\n",
    "# use of adam optimizer\n",
    "# accuracy is good metric for classification tasks\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# train the network\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=batch_size)\n",
    "\n",
    "# validate the model on test dataset to determine generalization\n",
    "_, acc = model.evaluate(x_test,\n",
    "                        y_test,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 256)               72960     \n",
      "                                                                 \n",
      " dense_244 (Dense)           (None, 10)                2570      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 75,530\n",
      "Trainable params: 75,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.7514 - accuracy: 0.7876\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.3231 - accuracy: 0.9059\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.2355 - accuracy: 0.9307\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.1969 - accuracy: 0.9420\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.1700 - accuracy: 0.9496\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.1516 - accuracy: 0.9554\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.1356 - accuracy: 0.9597\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.1250 - accuracy: 0.9629\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.1166 - accuracy: 0.9651\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.1097 - accuracy: 0.9675\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.1023 - accuracy: 0.9698\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.0968 - accuracy: 0.9710\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.0928 - accuracy: 0.9726\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.0888 - accuracy: 0.9729\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.0839 - accuracy: 0.9748\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0795 - accuracy: 0.9759\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.0765 - accuracy: 0.9763\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.0753 - accuracy: 0.9773\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0699 - accuracy: 0.9788\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0693 - accuracy: 0.9785\n",
      "\n",
      "Test accuracy: 98.1%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from  keras.models import Sequential\n",
    "from  keras.layers import Dense, Activation, SimpleRNN\n",
    "from  keras.utils import to_categorical, plot_model\n",
    "from  keras.datasets import mnist\n",
    "\n",
    "# load mnist dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# compute the number of labels\n",
    "num_labels = len(np.unique(y_train))\n",
    "\n",
    "# convert to one-hot vector\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# resize and normalize\n",
    "image_size = x_train.shape[1]\n",
    "x_train = np.reshape(x_train,[-1, image_size, image_size])\n",
    "x_test = np.reshape(x_test,[-1, image_size, image_size])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# network parameters\n",
    "input_shape = (image_size, image_size)\n",
    "batch_size = 128\n",
    "units = 256\n",
    "dropout = 0.2\n",
    "\n",
    "# model is RNN with 256 units, input is 28-dim vector 28 timesteps\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=units,\n",
    "                    dropout=dropout,\n",
    "                    input_shape=input_shape))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "# enable this if pydot can be installed\n",
    "# pip install pydot\n",
    "#plot_model(model, to_file='rnn-mnist.png', show_shapes=True)\n",
    "\n",
    "# loss function for one-hot vector\n",
    "# use of sgd optimizer\n",
    "# accuracy is good metric for classification tasks\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "# train the network\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=batch_size)\n",
    "\n",
    "_, acc = model.evaluate(x_test,\n",
    "                        y_test,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raja Babu\\AppData\\Local\\Temp\\ipykernel_20776\\1716227877.py:23: MatplotlibDeprecationWarning: The 'b' parameter of grid() has been renamed 'visible' since Matplotlib 3.5; support for the old name will be dropped two minor releases later.\n",
      "  plt.grid(b=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Utility for plotting a 2nd deg polynomial and\n",
    "its derivative\n",
    "'''\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('grayscale')\n",
    "x = np.arange(-1, 2, 0.1)\n",
    "c = [1, -1, -1]\n",
    "d = [2, -1]\n",
    "y = np.polyval(c, x)\n",
    "z = np.polyval(d, x)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(r'$y\\/\\/\\/and\\/\\/\\/\\frac{dy}{dx}$')\n",
    "plt.plot(x, y, label=r'$y=x^2 -x -1$')\n",
    "plt.plot(x, z, label=r'$\\frac{dy}{dx},\\/\\/\\/y_{min}\\/\\/at\\/\\/x=0.5$')\n",
    "plt.legend(loc=0)\n",
    "plt.grid(b=True)\n",
    "plt.savefig(\"sgd.png\")\n",
    "plt.show()\n",
    "plt.close('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 28, 28, 32)   320         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 28, 28, 32)   320         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 28, 28, 32)   0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 28, 28, 32)   0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 32)  0           ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 14, 14, 32)  0           ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 14, 14, 64)   18496       ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 14, 14, 64)   18496       ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 14, 14, 64)   0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 14, 14, 64)   0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 7, 7, 64)    0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 7, 7, 64)    0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 7, 7, 128)    73856       ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 7, 7, 128)    73856       ['max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 7, 7, 128)    0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 7, 7, 128)    0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 128)   0           ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 3, 3, 128)   0           ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3, 3, 256)    0           ['max_pooling2d_4[0][0]',        \n",
      "                                                                  'max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 2304)         0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 2304)         0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_245 (Dense)              (None, 10)           23050       ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 208,394\n",
      "Trainable params: 208,394\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 78s 41ms/step - loss: 0.1765 - accuracy: 0.9440 - val_loss: 0.1186 - val_accuracy: 0.9884\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 78s 41ms/step - loss: 0.0667 - accuracy: 0.9796 - val_loss: 0.0993 - val_accuracy: 0.9907\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 77s 41ms/step - loss: 0.0513 - accuracy: 0.9842 - val_loss: 0.0939 - val_accuracy: 0.9896\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 78s 42ms/step - loss: 0.0460 - accuracy: 0.9858 - val_loss: 0.0735 - val_accuracy: 0.9902\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 76s 41ms/step - loss: 0.0414 - accuracy: 0.9870 - val_loss: 0.0658 - val_accuracy: 0.9934\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 79s 42ms/step - loss: 0.0402 - accuracy: 0.9874 - val_loss: 0.0667 - val_accuracy: 0.9925\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 82s 44ms/step - loss: 0.0373 - accuracy: 0.9883 - val_loss: 0.0512 - val_accuracy: 0.9930\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 78s 42ms/step - loss: 0.0364 - accuracy: 0.9887 - val_loss: 0.0382 - val_accuracy: 0.9933\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 78s 42ms/step - loss: 0.0342 - accuracy: 0.9895 - val_loss: 0.0555 - val_accuracy: 0.9935\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 79s 42ms/step - loss: 0.0320 - accuracy: 0.9899 - val_loss: 0.0430 - val_accuracy: 0.9927\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 80s 42ms/step - loss: 0.0335 - accuracy: 0.9892 - val_loss: 0.0431 - val_accuracy: 0.9932\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0314 - accuracy: 0.9903 - val_loss: 0.0323 - val_accuracy: 0.9937\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 80s 43ms/step - loss: 0.0311 - accuracy: 0.9906 - val_loss: 0.0414 - val_accuracy: 0.9920\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 79s 42ms/step - loss: 0.0301 - accuracy: 0.9908 - val_loss: 0.0347 - val_accuracy: 0.9950\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 79s 42ms/step - loss: 0.0315 - accuracy: 0.9902 - val_loss: 0.0272 - val_accuracy: 0.9929\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 81s 43ms/step - loss: 0.0311 - accuracy: 0.9905 - val_loss: 0.0424 - val_accuracy: 0.9935\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 79s 42ms/step - loss: 0.0298 - accuracy: 0.9905 - val_loss: 0.0453 - val_accuracy: 0.9938\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 80s 42ms/step - loss: 0.0300 - accuracy: 0.9905 - val_loss: 0.0354 - val_accuracy: 0.9936\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 80s 42ms/step - loss: 0.0302 - accuracy: 0.9909 - val_loss: 0.0396 - val_accuracy: 0.9931\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 80s 43ms/step - loss: 0.0289 - accuracy: 0.9909 - val_loss: 0.0338 - val_accuracy: 0.9928\n",
      "\n",
      "Test accuracy: 99.3%\n"
     ]
    }
   ],
   "source": [
    "'''Implements a Y-Network using Functional API\n",
    "\n",
    "~99.3% test accuracy\n",
    "'''\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from  keras.layers import Dense, Dropout, Input\n",
    "from  keras.layers import Conv2D, MaxPooling2D\n",
    "from  keras.layers import Flatten, concatenate\n",
    "from  keras.models import Model\n",
    "from  keras.datasets import mnist\n",
    "from  keras.utils import to_categorical\n",
    "from  keras.utils import plot_model\n",
    "\n",
    "# load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# from sparse label to categorical\n",
    "num_labels = len(np.unique(y_train))\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# reshape and normalize input images\n",
    "image_size = x_train.shape[1]\n",
    "x_train = np.reshape(x_train,[-1, image_size, image_size, 1])\n",
    "x_test = np.reshape(x_test,[-1, image_size, image_size, 1])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# network parameters\n",
    "input_shape = (image_size, image_size, 1)\n",
    "batch_size = 32\n",
    "kernel_size = 3\n",
    "dropout = 0.4\n",
    "n_filters = 32\n",
    "\n",
    "# left branch of Y network\n",
    "left_inputs = Input(shape=input_shape)\n",
    "x = left_inputs\n",
    "filters = n_filters\n",
    "# 3 layers of Conv2D-Dropout-MaxPooling2D\n",
    "# number of filters doubles after each layer (32-64-128)\n",
    "for i in range(3):\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               padding='same',\n",
    "               activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    filters *= 2\n",
    "\n",
    "# right branch of Y network\n",
    "right_inputs = Input(shape=input_shape)\n",
    "y = right_inputs\n",
    "filters = n_filters\n",
    "# 3 layers of Conv2D-Dropout-MaxPooling2D\n",
    "# number of filters doubles after each layer (32-64-128)\n",
    "for i in range(3):\n",
    "    y = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               padding='same',\n",
    "               activation='relu',\n",
    "               dilation_rate=2)(y)\n",
    "    y = Dropout(dropout)(y)\n",
    "    y = MaxPooling2D()(y)\n",
    "    filters *= 2\n",
    "\n",
    "# merge left and right branches outputs\n",
    "y = concatenate([x, y])\n",
    "# feature maps to vector before connecting to Dense \n",
    "y = Flatten()(y)\n",
    "y = Dropout(dropout)(y)\n",
    "outputs = Dense(num_labels, activation='softmax')(y)\n",
    "\n",
    "# build the model in functional API\n",
    "model = Model([left_inputs, right_inputs], outputs)\n",
    "\n",
    "# verify the model using graph\n",
    "# enable this if pydot can be installed\n",
    "# pip install pydot\n",
    "#plot_model(model, to_file='cnn-y-network.png', show_shapes=True)\n",
    "\n",
    "# verify the model using layer text description\n",
    "model.summary()\n",
    "\n",
    "# classifier loss, Adam optimizer, classifier accuracy\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train the model with input images and labels\n",
    "model.fit([x_train, x_train],\n",
    "          y_train, \n",
    "          validation_data=([x_test, x_test], y_test),\n",
    "          epochs=20,\n",
    "          batch_size=batch_size)\n",
    "\n",
    "# model accuracy on test dataset\n",
    "score = model.evaluate([x_test, x_test],\n",
    "                       y_test,\n",
    "                       batch_size=batch_size,\n",
    "                       verbose=0)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "y_train shape: (50000, 1)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_198 (Batch  (None, 32, 32, 3)   12          ['input_3[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_194 (Activation)    (None, 32, 32, 3)    0           ['batch_normalization_198[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_198 (Conv2D)            (None, 32, 32, 24)   672         ['activation_194[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_98 (Concatenate)   (None, 32, 32, 27)   0           ['input_3[0][0]',                \n",
      "                                                                  'conv2d_198[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_199 (Batch  (None, 32, 32, 27)  108         ['concatenate_98[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_195 (Activation)    (None, 32, 32, 27)   0           ['batch_normalization_199[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_199 (Conv2D)            (None, 32, 32, 48)   1344        ['activation_195[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_200 (Batch  (None, 32, 32, 48)  192         ['conv2d_199[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_196 (Activation)    (None, 32, 32, 48)   0           ['batch_normalization_200[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_200 (Conv2D)            (None, 32, 32, 12)   5196        ['activation_196[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_99 (Concatenate)   (None, 32, 32, 39)   0           ['concatenate_98[0][0]',         \n",
      "                                                                  'conv2d_200[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_201 (Batch  (None, 32, 32, 39)  156         ['concatenate_99[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_197 (Activation)    (None, 32, 32, 39)   0           ['batch_normalization_201[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_201 (Conv2D)            (None, 32, 32, 48)   1920        ['activation_197[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_202 (Batch  (None, 32, 32, 48)  192         ['conv2d_201[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_198 (Activation)    (None, 32, 32, 48)   0           ['batch_normalization_202[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_202 (Conv2D)            (None, 32, 32, 12)   5196        ['activation_198[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_100 (Concatenate)  (None, 32, 32, 51)   0           ['concatenate_99[0][0]',         \n",
      "                                                                  'conv2d_202[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_203 (Batch  (None, 32, 32, 51)  204         ['concatenate_100[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_199 (Activation)    (None, 32, 32, 51)   0           ['batch_normalization_203[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_203 (Conv2D)            (None, 32, 32, 48)   2496        ['activation_199[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_204 (Batch  (None, 32, 32, 48)  192         ['conv2d_203[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_200 (Activation)    (None, 32, 32, 48)   0           ['batch_normalization_204[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_204 (Conv2D)            (None, 32, 32, 12)   5196        ['activation_200[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_101 (Concatenate)  (None, 32, 32, 63)   0           ['concatenate_100[0][0]',        \n",
      "                                                                  'conv2d_204[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_205 (Batch  (None, 32, 32, 63)  252         ['concatenate_101[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_201 (Activation)    (None, 32, 32, 63)   0           ['batch_normalization_205[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_205 (Conv2D)            (None, 32, 32, 48)   3072        ['activation_201[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_206 (Batch  (None, 32, 32, 48)  192         ['conv2d_205[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_202 (Activation)    (None, 32, 32, 48)   0           ['batch_normalization_206[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_206 (Conv2D)            (None, 32, 32, 12)   5196        ['activation_202[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_102 (Concatenate)  (None, 32, 32, 75)   0           ['concatenate_101[0][0]',        \n",
      "                                                                  'conv2d_206[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_207 (Batch  (None, 32, 32, 75)  300         ['concatenate_102[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_203 (Activation)    (None, 32, 32, 75)   0           ['batch_normalization_207[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_207 (Conv2D)            (None, 32, 32, 48)   3648        ['activation_203[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_208 (Batch  (None, 32, 32, 48)  192         ['conv2d_207[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_204 (Activation)    (None, 32, 32, 48)   0           ['batch_normalization_208[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_208 (Conv2D)            (None, 32, 32, 12)   5196        ['activation_204[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_103 (Concatenate)  (None, 32, 32, 87)   0           ['concatenate_102[0][0]',        \n",
      "                                                                  'conv2d_208[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_209 (Batch  (None, 32, 32, 87)  348         ['concatenate_103[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_205 (Activation)    (None, 32, 32, 87)   0           ['batch_normalization_209[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_209 (Conv2D)            (None, 32, 32, 48)   4224        ['activation_205[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_210 (Batch  (None, 32, 32, 48)  192         ['conv2d_209[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_206 (Activation)    (None, 32, 32, 48)   0           ['batch_normalization_210[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_210 (Conv2D)            (None, 32, 32, 12)   5196        ['activation_206[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_104 (Concatenate)  (None, 32, 32, 99)   0           ['concatenate_103[0][0]',        \n",
      "                                                                  'conv2d_210[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_211 (Batch  (None, 32, 32, 99)  396         ['concatenate_104[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_207 (Activation)    (None, 32, 32, 99)   0           ['batch_normalization_211[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_211 (Conv2D)            (None, 32, 32, 48)   4800        ['activation_207[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_212 (Batch  (None, 32, 32, 48)  192         ['conv2d_211[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_208 (Activation)    (None, 32, 32, 48)   0           ['batch_normalization_212[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_212 (Conv2D)            (None, 32, 32, 12)   5196        ['activation_208[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_105 (Concatenate)  (None, 32, 32, 111)  0           ['concatenate_104[0][0]',        \n",
      "                                                                  'conv2d_212[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_213 (Batch  (None, 32, 32, 111)  444        ['concatenate_105[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_209 (Activation)    (None, 32, 32, 111)  0           ['batch_normalization_213[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_213 (Conv2D)            (None, 32, 32, 48)   5376        ['activation_209[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_214 (Batch  (None, 32, 32, 48)  192         ['conv2d_213[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_210 (Activation)    (None, 32, 32, 48)   0           ['batch_normalization_214[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_214 (Conv2D)            (None, 32, 32, 12)   5196        ['activation_210[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_106 (Concatenate)  (None, 32, 32, 123)  0           ['concatenate_105[0][0]',        \n",
      "                                                                  'conv2d_214[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_215 (Batch  (None, 32, 32, 123)  492        ['concatenate_106[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_211 (Activation)    (None, 32, 32, 123)  0           ['batch_normalization_215[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_215 (Conv2D)            (None, 32, 32, 48)   5952        ['activation_211[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_216 (Batch  (None, 32, 32, 48)  192         ['conv2d_215[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_212 (Activation)    (None, 32, 32, 48)   0           ['batch_normalization_216[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_216 (Conv2D)            (None, 32, 32, 12)   5196        ['activation_212[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_107 (Concatenate)  (None, 32, 32, 135)  0           ['concatenate_106[0][0]',        \n",
      "                                                                  'conv2d_216[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_217 (Batch  (None, 32, 32, 135)  540        ['concatenate_107[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_213 (Activation)    (None, 32, 32, 135)  0           ['batch_normalization_217[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_217 (Conv2D)            (None, 32, 32, 48)   6528        ['activation_213[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_218 (Batch  (None, 32, 32, 48)  192         ['conv2d_217[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_214 (Activation)    (None, 32, 32, 48)   0           ['batch_normalization_218[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_218 (Conv2D)            (None, 32, 32, 12)   5196        ['activation_214[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_108 (Concatenate)  (None, 32, 32, 147)  0           ['concatenate_107[0][0]',        \n",
      "                                                                  'conv2d_218[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_219 (Batch  (None, 32, 32, 147)  588        ['concatenate_108[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_215 (Activation)    (None, 32, 32, 147)  0           ['batch_normalization_219[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_219 (Conv2D)            (None, 32, 32, 48)   7104        ['activation_215[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_220 (Batch  (None, 32, 32, 48)  192         ['conv2d_219[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_216 (Activation)    (None, 32, 32, 48)   0           ['batch_normalization_220[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_220 (Conv2D)            (None, 32, 32, 12)   5196        ['activation_216[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_109 (Concatenate)  (None, 32, 32, 159)  0           ['concatenate_108[0][0]',        \n",
      "                                                                  'conv2d_220[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_221 (Batch  (None, 32, 32, 159)  636        ['concatenate_109[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_217 (Activation)    (None, 32, 32, 159)  0           ['batch_normalization_221[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_221 (Conv2D)            (None, 32, 32, 48)   7680        ['activation_217[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_222 (Batch  (None, 32, 32, 48)  192         ['conv2d_221[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_218 (Activation)    (None, 32, 32, 48)   0           ['batch_normalization_222[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_222 (Conv2D)            (None, 32, 32, 12)   5196        ['activation_218[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_110 (Concatenate)  (None, 32, 32, 171)  0           ['concatenate_109[0][0]',        \n",
      "                                                                  'conv2d_222[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_223 (Batch  (None, 32, 32, 171)  684        ['concatenate_110[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_219 (Activation)    (None, 32, 32, 171)  0           ['batch_normalization_223[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_223 (Conv2D)            (None, 32, 32, 48)   8256        ['activation_219[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_224 (Batch  (None, 32, 32, 48)  192         ['conv2d_223[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_220 (Activation)    (None, 32, 32, 48)   0           ['batch_normalization_224[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_224 (Conv2D)            (None, 32, 32, 12)   5196        ['activation_220[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_111 (Concatenate)  (None, 32, 32, 183)  0           ['concatenate_110[0][0]',        \n",
      "                                                                  'conv2d_224[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_225 (Batch  (None, 32, 32, 183)  732        ['concatenate_111[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_221 (Activation)    (None, 32, 32, 183)  0           ['batch_normalization_225[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_225 (Conv2D)            (None, 32, 32, 48)   8832        ['activation_221[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_226 (Batch  (None, 32, 32, 48)  192         ['conv2d_225[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_222 (Activation)    (None, 32, 32, 48)   0           ['batch_normalization_226[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_226 (Conv2D)            (None, 32, 32, 12)   5196        ['activation_222[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_112 (Concatenate)  (None, 32, 32, 195)  0           ['concatenate_111[0][0]',        \n",
      "                                                                  'conv2d_226[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_227 (Batch  (None, 32, 32, 195)  780        ['concatenate_112[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_223 (Activation)    (None, 32, 32, 195)  0           ['batch_normalization_227[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_227 (Conv2D)            (None, 32, 32, 48)   9408        ['activation_223[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_228 (Batch  (None, 32, 32, 48)  192         ['conv2d_227[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_224 (Activation)    (None, 32, 32, 48)   0           ['batch_normalization_228[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_228 (Conv2D)            (None, 32, 32, 12)   5196        ['activation_224[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_113 (Concatenate)  (None, 32, 32, 207)  0           ['concatenate_112[0][0]',        \n",
      "                                                                  'conv2d_228[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_229 (Batch  (None, 32, 32, 207)  828        ['concatenate_113[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_225 (Activation)    (None, 32, 32, 207)  0           ['batch_normalization_229[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_229 (Conv2D)            (None, 32, 32, 48)   9984        ['activation_225[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_230 (Batch  (None, 32, 32, 48)  192         ['conv2d_229[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_226 (Activation)    (None, 32, 32, 48)   0           ['batch_normalization_230[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_230 (Conv2D)            (None, 32, 32, 12)   5196        ['activation_226[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_114 (Concatenate)  (None, 32, 32, 219)  0           ['concatenate_113[0][0]',        \n",
      "                                                                  'conv2d_230[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_231 (Batch  (None, 32, 32, 219)  876        ['concatenate_114[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_231 (Conv2D)            (None, 32, 32, 108)  23760       ['batch_normalization_231[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_6 (AveragePo  (None, 16, 16, 108)  0          ['conv2d_231[0][0]']             \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_232 (Batch  (None, 16, 16, 108)  432        ['average_pooling2d_6[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_227 (Activation)    (None, 16, 16, 108)  0           ['batch_normalization_232[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_232 (Conv2D)            (None, 16, 16, 48)   5232        ['activation_227[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_233 (Batch  (None, 16, 16, 48)  192         ['conv2d_232[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_228 (Activation)    (None, 16, 16, 48)   0           ['batch_normalization_233[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_233 (Conv2D)            (None, 16, 16, 12)   5196        ['activation_228[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_115 (Concatenate)  (None, 16, 16, 120)  0           ['average_pooling2d_6[0][0]',    \n",
      "                                                                  'conv2d_233[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_234 (Batch  (None, 16, 16, 120)  480        ['concatenate_115[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_229 (Activation)    (None, 16, 16, 120)  0           ['batch_normalization_234[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_234 (Conv2D)            (None, 16, 16, 48)   5808        ['activation_229[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_235 (Batch  (None, 16, 16, 48)  192         ['conv2d_234[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_230 (Activation)    (None, 16, 16, 48)   0           ['batch_normalization_235[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_235 (Conv2D)            (None, 16, 16, 12)   5196        ['activation_230[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_116 (Concatenate)  (None, 16, 16, 132)  0           ['concatenate_115[0][0]',        \n",
      "                                                                  'conv2d_235[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_236 (Batch  (None, 16, 16, 132)  528        ['concatenate_116[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_231 (Activation)    (None, 16, 16, 132)  0           ['batch_normalization_236[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_236 (Conv2D)            (None, 16, 16, 48)   6384        ['activation_231[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_237 (Batch  (None, 16, 16, 48)  192         ['conv2d_236[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_232 (Activation)    (None, 16, 16, 48)   0           ['batch_normalization_237[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_237 (Conv2D)            (None, 16, 16, 12)   5196        ['activation_232[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_117 (Concatenate)  (None, 16, 16, 144)  0           ['concatenate_116[0][0]',        \n",
      "                                                                  'conv2d_237[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_238 (Batch  (None, 16, 16, 144)  576        ['concatenate_117[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_233 (Activation)    (None, 16, 16, 144)  0           ['batch_normalization_238[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_238 (Conv2D)            (None, 16, 16, 48)   6960        ['activation_233[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_239 (Batch  (None, 16, 16, 48)  192         ['conv2d_238[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_234 (Activation)    (None, 16, 16, 48)   0           ['batch_normalization_239[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_239 (Conv2D)            (None, 16, 16, 12)   5196        ['activation_234[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_118 (Concatenate)  (None, 16, 16, 156)  0           ['concatenate_117[0][0]',        \n",
      "                                                                  'conv2d_239[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_240 (Batch  (None, 16, 16, 156)  624        ['concatenate_118[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_235 (Activation)    (None, 16, 16, 156)  0           ['batch_normalization_240[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_240 (Conv2D)            (None, 16, 16, 48)   7536        ['activation_235[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_241 (Batch  (None, 16, 16, 48)  192         ['conv2d_240[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_236 (Activation)    (None, 16, 16, 48)   0           ['batch_normalization_241[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_241 (Conv2D)            (None, 16, 16, 12)   5196        ['activation_236[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_119 (Concatenate)  (None, 16, 16, 168)  0           ['concatenate_118[0][0]',        \n",
      "                                                                  'conv2d_241[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_242 (Batch  (None, 16, 16, 168)  672        ['concatenate_119[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_237 (Activation)    (None, 16, 16, 168)  0           ['batch_normalization_242[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_242 (Conv2D)            (None, 16, 16, 48)   8112        ['activation_237[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_243 (Batch  (None, 16, 16, 48)  192         ['conv2d_242[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_238 (Activation)    (None, 16, 16, 48)   0           ['batch_normalization_243[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_243 (Conv2D)            (None, 16, 16, 12)   5196        ['activation_238[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_120 (Concatenate)  (None, 16, 16, 180)  0           ['concatenate_119[0][0]',        \n",
      "                                                                  'conv2d_243[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_244 (Batch  (None, 16, 16, 180)  720        ['concatenate_120[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_239 (Activation)    (None, 16, 16, 180)  0           ['batch_normalization_244[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_244 (Conv2D)            (None, 16, 16, 48)   8688        ['activation_239[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_245 (Batch  (None, 16, 16, 48)  192         ['conv2d_244[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_240 (Activation)    (None, 16, 16, 48)   0           ['batch_normalization_245[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_245 (Conv2D)            (None, 16, 16, 12)   5196        ['activation_240[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_121 (Concatenate)  (None, 16, 16, 192)  0           ['concatenate_120[0][0]',        \n",
      "                                                                  'conv2d_245[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_246 (Batch  (None, 16, 16, 192)  768        ['concatenate_121[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_241 (Activation)    (None, 16, 16, 192)  0           ['batch_normalization_246[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_246 (Conv2D)            (None, 16, 16, 48)   9264        ['activation_241[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_247 (Batch  (None, 16, 16, 48)  192         ['conv2d_246[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_242 (Activation)    (None, 16, 16, 48)   0           ['batch_normalization_247[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_247 (Conv2D)            (None, 16, 16, 12)   5196        ['activation_242[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_122 (Concatenate)  (None, 16, 16, 204)  0           ['concatenate_121[0][0]',        \n",
      "                                                                  'conv2d_247[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_248 (Batch  (None, 16, 16, 204)  816        ['concatenate_122[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_243 (Activation)    (None, 16, 16, 204)  0           ['batch_normalization_248[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_248 (Conv2D)            (None, 16, 16, 48)   9840        ['activation_243[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_249 (Batch  (None, 16, 16, 48)  192         ['conv2d_248[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_244 (Activation)    (None, 16, 16, 48)   0           ['batch_normalization_249[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_249 (Conv2D)            (None, 16, 16, 12)   5196        ['activation_244[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_123 (Concatenate)  (None, 16, 16, 216)  0           ['concatenate_122[0][0]',        \n",
      "                                                                  'conv2d_249[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_250 (Batch  (None, 16, 16, 216)  864        ['concatenate_123[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_245 (Activation)    (None, 16, 16, 216)  0           ['batch_normalization_250[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_250 (Conv2D)            (None, 16, 16, 48)   10416       ['activation_245[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_251 (Batch  (None, 16, 16, 48)  192         ['conv2d_250[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_246 (Activation)    (None, 16, 16, 48)   0           ['batch_normalization_251[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_251 (Conv2D)            (None, 16, 16, 12)   5196        ['activation_246[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_124 (Concatenate)  (None, 16, 16, 228)  0           ['concatenate_123[0][0]',        \n",
      "                                                                  'conv2d_251[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_252 (Batch  (None, 16, 16, 228)  912        ['concatenate_124[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_247 (Activation)    (None, 16, 16, 228)  0           ['batch_normalization_252[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_252 (Conv2D)            (None, 16, 16, 48)   10992       ['activation_247[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_253 (Batch  (None, 16, 16, 48)  192         ['conv2d_252[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_248 (Activation)    (None, 16, 16, 48)   0           ['batch_normalization_253[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_253 (Conv2D)            (None, 16, 16, 12)   5196        ['activation_248[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_125 (Concatenate)  (None, 16, 16, 240)  0           ['concatenate_124[0][0]',        \n",
      "                                                                  'conv2d_253[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_254 (Batch  (None, 16, 16, 240)  960        ['concatenate_125[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_249 (Activation)    (None, 16, 16, 240)  0           ['batch_normalization_254[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_254 (Conv2D)            (None, 16, 16, 48)   11568       ['activation_249[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_255 (Batch  (None, 16, 16, 48)  192         ['conv2d_254[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_250 (Activation)    (None, 16, 16, 48)   0           ['batch_normalization_255[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_255 (Conv2D)            (None, 16, 16, 12)   5196        ['activation_250[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_126 (Concatenate)  (None, 16, 16, 252)  0           ['concatenate_125[0][0]',        \n",
      "                                                                  'conv2d_255[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_256 (Batch  (None, 16, 16, 252)  1008       ['concatenate_126[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_251 (Activation)    (None, 16, 16, 252)  0           ['batch_normalization_256[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_256 (Conv2D)            (None, 16, 16, 48)   12144       ['activation_251[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_257 (Batch  (None, 16, 16, 48)  192         ['conv2d_256[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_252 (Activation)    (None, 16, 16, 48)   0           ['batch_normalization_257[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_257 (Conv2D)            (None, 16, 16, 12)   5196        ['activation_252[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_127 (Concatenate)  (None, 16, 16, 264)  0           ['concatenate_126[0][0]',        \n",
      "                                                                  'conv2d_257[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_258 (Batch  (None, 16, 16, 264)  1056       ['concatenate_127[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_253 (Activation)    (None, 16, 16, 264)  0           ['batch_normalization_258[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_258 (Conv2D)            (None, 16, 16, 48)   12720       ['activation_253[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_259 (Batch  (None, 16, 16, 48)  192         ['conv2d_258[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_254 (Activation)    (None, 16, 16, 48)   0           ['batch_normalization_259[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_259 (Conv2D)            (None, 16, 16, 12)   5196        ['activation_254[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_128 (Concatenate)  (None, 16, 16, 276)  0           ['concatenate_127[0][0]',        \n",
      "                                                                  'conv2d_259[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_260 (Batch  (None, 16, 16, 276)  1104       ['concatenate_128[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_255 (Activation)    (None, 16, 16, 276)  0           ['batch_normalization_260[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_260 (Conv2D)            (None, 16, 16, 48)   13296       ['activation_255[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_261 (Batch  (None, 16, 16, 48)  192         ['conv2d_260[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_256 (Activation)    (None, 16, 16, 48)   0           ['batch_normalization_261[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_261 (Conv2D)            (None, 16, 16, 12)   5196        ['activation_256[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_129 (Concatenate)  (None, 16, 16, 288)  0           ['concatenate_128[0][0]',        \n",
      "                                                                  'conv2d_261[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_262 (Batch  (None, 16, 16, 288)  1152       ['concatenate_129[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_257 (Activation)    (None, 16, 16, 288)  0           ['batch_normalization_262[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_262 (Conv2D)            (None, 16, 16, 48)   13872       ['activation_257[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_263 (Batch  (None, 16, 16, 48)  192         ['conv2d_262[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_258 (Activation)    (None, 16, 16, 48)   0           ['batch_normalization_263[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_263 (Conv2D)            (None, 16, 16, 12)   5196        ['activation_258[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_130 (Concatenate)  (None, 16, 16, 300)  0           ['concatenate_129[0][0]',        \n",
      "                                                                  'conv2d_263[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_264 (Batch  (None, 16, 16, 300)  1200       ['concatenate_130[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_264 (Conv2D)            (None, 16, 16, 150)  45150       ['batch_normalization_264[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_7 (AveragePo  (None, 8, 8, 150)   0           ['conv2d_264[0][0]']             \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_265 (Batch  (None, 8, 8, 150)   600         ['average_pooling2d_7[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_259 (Activation)    (None, 8, 8, 150)    0           ['batch_normalization_265[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_265 (Conv2D)            (None, 8, 8, 48)     7248        ['activation_259[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_266 (Batch  (None, 8, 8, 48)    192         ['conv2d_265[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_260 (Activation)    (None, 8, 8, 48)     0           ['batch_normalization_266[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_266 (Conv2D)            (None, 8, 8, 12)     5196        ['activation_260[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_131 (Concatenate)  (None, 8, 8, 162)    0           ['average_pooling2d_7[0][0]',    \n",
      "                                                                  'conv2d_266[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_267 (Batch  (None, 8, 8, 162)   648         ['concatenate_131[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_261 (Activation)    (None, 8, 8, 162)    0           ['batch_normalization_267[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_267 (Conv2D)            (None, 8, 8, 48)     7824        ['activation_261[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_268 (Batch  (None, 8, 8, 48)    192         ['conv2d_267[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_262 (Activation)    (None, 8, 8, 48)     0           ['batch_normalization_268[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_268 (Conv2D)            (None, 8, 8, 12)     5196        ['activation_262[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_132 (Concatenate)  (None, 8, 8, 174)    0           ['concatenate_131[0][0]',        \n",
      "                                                                  'conv2d_268[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_269 (Batch  (None, 8, 8, 174)   696         ['concatenate_132[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_263 (Activation)    (None, 8, 8, 174)    0           ['batch_normalization_269[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_269 (Conv2D)            (None, 8, 8, 48)     8400        ['activation_263[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_270 (Batch  (None, 8, 8, 48)    192         ['conv2d_269[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_264 (Activation)    (None, 8, 8, 48)     0           ['batch_normalization_270[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_270 (Conv2D)            (None, 8, 8, 12)     5196        ['activation_264[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_133 (Concatenate)  (None, 8, 8, 186)    0           ['concatenate_132[0][0]',        \n",
      "                                                                  'conv2d_270[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_271 (Batch  (None, 8, 8, 186)   744         ['concatenate_133[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_265 (Activation)    (None, 8, 8, 186)    0           ['batch_normalization_271[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_271 (Conv2D)            (None, 8, 8, 48)     8976        ['activation_265[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_272 (Batch  (None, 8, 8, 48)    192         ['conv2d_271[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_266 (Activation)    (None, 8, 8, 48)     0           ['batch_normalization_272[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_272 (Conv2D)            (None, 8, 8, 12)     5196        ['activation_266[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_134 (Concatenate)  (None, 8, 8, 198)    0           ['concatenate_133[0][0]',        \n",
      "                                                                  'conv2d_272[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_273 (Batch  (None, 8, 8, 198)   792         ['concatenate_134[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_267 (Activation)    (None, 8, 8, 198)    0           ['batch_normalization_273[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_273 (Conv2D)            (None, 8, 8, 48)     9552        ['activation_267[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_274 (Batch  (None, 8, 8, 48)    192         ['conv2d_273[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_268 (Activation)    (None, 8, 8, 48)     0           ['batch_normalization_274[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_274 (Conv2D)            (None, 8, 8, 12)     5196        ['activation_268[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_135 (Concatenate)  (None, 8, 8, 210)    0           ['concatenate_134[0][0]',        \n",
      "                                                                  'conv2d_274[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_275 (Batch  (None, 8, 8, 210)   840         ['concatenate_135[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_269 (Activation)    (None, 8, 8, 210)    0           ['batch_normalization_275[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_275 (Conv2D)            (None, 8, 8, 48)     10128       ['activation_269[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_276 (Batch  (None, 8, 8, 48)    192         ['conv2d_275[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_270 (Activation)    (None, 8, 8, 48)     0           ['batch_normalization_276[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_276 (Conv2D)            (None, 8, 8, 12)     5196        ['activation_270[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_136 (Concatenate)  (None, 8, 8, 222)    0           ['concatenate_135[0][0]',        \n",
      "                                                                  'conv2d_276[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_277 (Batch  (None, 8, 8, 222)   888         ['concatenate_136[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_271 (Activation)    (None, 8, 8, 222)    0           ['batch_normalization_277[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_277 (Conv2D)            (None, 8, 8, 48)     10704       ['activation_271[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_278 (Batch  (None, 8, 8, 48)    192         ['conv2d_277[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_272 (Activation)    (None, 8, 8, 48)     0           ['batch_normalization_278[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_278 (Conv2D)            (None, 8, 8, 12)     5196        ['activation_272[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_137 (Concatenate)  (None, 8, 8, 234)    0           ['concatenate_136[0][0]',        \n",
      "                                                                  'conv2d_278[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_279 (Batch  (None, 8, 8, 234)   936         ['concatenate_137[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_273 (Activation)    (None, 8, 8, 234)    0           ['batch_normalization_279[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_279 (Conv2D)            (None, 8, 8, 48)     11280       ['activation_273[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_280 (Batch  (None, 8, 8, 48)    192         ['conv2d_279[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_274 (Activation)    (None, 8, 8, 48)     0           ['batch_normalization_280[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_280 (Conv2D)            (None, 8, 8, 12)     5196        ['activation_274[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_138 (Concatenate)  (None, 8, 8, 246)    0           ['concatenate_137[0][0]',        \n",
      "                                                                  'conv2d_280[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_281 (Batch  (None, 8, 8, 246)   984         ['concatenate_138[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_275 (Activation)    (None, 8, 8, 246)    0           ['batch_normalization_281[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_281 (Conv2D)            (None, 8, 8, 48)     11856       ['activation_275[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_282 (Batch  (None, 8, 8, 48)    192         ['conv2d_281[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_276 (Activation)    (None, 8, 8, 48)     0           ['batch_normalization_282[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_282 (Conv2D)            (None, 8, 8, 12)     5196        ['activation_276[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_139 (Concatenate)  (None, 8, 8, 258)    0           ['concatenate_138[0][0]',        \n",
      "                                                                  'conv2d_282[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_283 (Batch  (None, 8, 8, 258)   1032        ['concatenate_139[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_277 (Activation)    (None, 8, 8, 258)    0           ['batch_normalization_283[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_283 (Conv2D)            (None, 8, 8, 48)     12432       ['activation_277[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_284 (Batch  (None, 8, 8, 48)    192         ['conv2d_283[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_278 (Activation)    (None, 8, 8, 48)     0           ['batch_normalization_284[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_284 (Conv2D)            (None, 8, 8, 12)     5196        ['activation_278[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_140 (Concatenate)  (None, 8, 8, 270)    0           ['concatenate_139[0][0]',        \n",
      "                                                                  'conv2d_284[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_285 (Batch  (None, 8, 8, 270)   1080        ['concatenate_140[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_279 (Activation)    (None, 8, 8, 270)    0           ['batch_normalization_285[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_285 (Conv2D)            (None, 8, 8, 48)     13008       ['activation_279[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_286 (Batch  (None, 8, 8, 48)    192         ['conv2d_285[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_280 (Activation)    (None, 8, 8, 48)     0           ['batch_normalization_286[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_286 (Conv2D)            (None, 8, 8, 12)     5196        ['activation_280[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_141 (Concatenate)  (None, 8, 8, 282)    0           ['concatenate_140[0][0]',        \n",
      "                                                                  'conv2d_286[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_287 (Batch  (None, 8, 8, 282)   1128        ['concatenate_141[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_281 (Activation)    (None, 8, 8, 282)    0           ['batch_normalization_287[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_287 (Conv2D)            (None, 8, 8, 48)     13584       ['activation_281[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_288 (Batch  (None, 8, 8, 48)    192         ['conv2d_287[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_282 (Activation)    (None, 8, 8, 48)     0           ['batch_normalization_288[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_288 (Conv2D)            (None, 8, 8, 12)     5196        ['activation_282[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_142 (Concatenate)  (None, 8, 8, 294)    0           ['concatenate_141[0][0]',        \n",
      "                                                                  'conv2d_288[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_289 (Batch  (None, 8, 8, 294)   1176        ['concatenate_142[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_283 (Activation)    (None, 8, 8, 294)    0           ['batch_normalization_289[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_289 (Conv2D)            (None, 8, 8, 48)     14160       ['activation_283[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_290 (Batch  (None, 8, 8, 48)    192         ['conv2d_289[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_284 (Activation)    (None, 8, 8, 48)     0           ['batch_normalization_290[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_290 (Conv2D)            (None, 8, 8, 12)     5196        ['activation_284[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_143 (Concatenate)  (None, 8, 8, 306)    0           ['concatenate_142[0][0]',        \n",
      "                                                                  'conv2d_290[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_291 (Batch  (None, 8, 8, 306)   1224        ['concatenate_143[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_285 (Activation)    (None, 8, 8, 306)    0           ['batch_normalization_291[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_291 (Conv2D)            (None, 8, 8, 48)     14736       ['activation_285[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_292 (Batch  (None, 8, 8, 48)    192         ['conv2d_291[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_286 (Activation)    (None, 8, 8, 48)     0           ['batch_normalization_292[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_292 (Conv2D)            (None, 8, 8, 12)     5196        ['activation_286[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_144 (Concatenate)  (None, 8, 8, 318)    0           ['concatenate_143[0][0]',        \n",
      "                                                                  'conv2d_292[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_293 (Batch  (None, 8, 8, 318)   1272        ['concatenate_144[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_287 (Activation)    (None, 8, 8, 318)    0           ['batch_normalization_293[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_293 (Conv2D)            (None, 8, 8, 48)     15312       ['activation_287[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_294 (Batch  (None, 8, 8, 48)    192         ['conv2d_293[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_288 (Activation)    (None, 8, 8, 48)     0           ['batch_normalization_294[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_294 (Conv2D)            (None, 8, 8, 12)     5196        ['activation_288[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_145 (Concatenate)  (None, 8, 8, 330)    0           ['concatenate_144[0][0]',        \n",
      "                                                                  'conv2d_294[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_295 (Batch  (None, 8, 8, 330)   1320        ['concatenate_145[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_289 (Activation)    (None, 8, 8, 330)    0           ['batch_normalization_295[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_295 (Conv2D)            (None, 8, 8, 48)     15888       ['activation_289[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_296 (Batch  (None, 8, 8, 48)    192         ['conv2d_295[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_290 (Activation)    (None, 8, 8, 48)     0           ['batch_normalization_296[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_296 (Conv2D)            (None, 8, 8, 12)     5196        ['activation_290[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_146 (Concatenate)  (None, 8, 8, 342)    0           ['concatenate_145[0][0]',        \n",
      "                                                                  'conv2d_296[0][0]']             \n",
      "                                                                                                  \n",
      " average_pooling2d_8 (AveragePo  (None, 1, 1, 342)   0           ['concatenate_146[0][0]']        \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 342)          0           ['average_pooling2d_8[0][0]']    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 10)           3430        ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 797,788\n",
      "Trainable params: 774,376\n",
      "Non-trainable params: 23,412\n",
      "__________________________________________________________________________________________________\n",
      "Using real-time data augmentation.\n",
      "Learning rate:  0.001\n",
      "Epoch 1/200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Trains a 100-Layer DenseNet on the CIFAR10 dataset.\n",
    "\n",
    "With data augmentation:\n",
    "Greater than 93.55% test accuracy in 200 epochs\n",
    "225sec per epoch on GTX 1080Ti\n",
    "\n",
    "Densely Connected Convolutional Networks\n",
    "https://arxiv.org/pdf/1608.06993.pdf\n",
    "http://openaccess.thecvf.com/content_cvpr_2017/papers/\n",
    "    Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf\n",
    "Network below is similar to 100-Layer DenseNet-BC (k=12)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from  keras.layers import Dense, Conv2D, BatchNormalization\n",
    "from  keras.layers import MaxPooling2D, AveragePooling2D\n",
    "from  keras.layers import Input, Flatten, Dropout\n",
    "from  keras.layers import concatenate, Activation\n",
    "from  keras.optimizers import RMSprop\n",
    "from  keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from  keras.callbacks import LearningRateScheduler\n",
    "from  keras.preprocessing.image import ImageDataGenerator\n",
    "from  keras.models import Model\n",
    "from  keras.datasets import cifar10\n",
    "from  keras.utils import plot_model\n",
    "from  keras.utils import to_categorical\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# training parameters\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "data_augmentation = True\n",
    "\n",
    "# network parameters\n",
    "num_classes = 10\n",
    "num_dense_blocks = 3\n",
    "use_max_pool = False\n",
    "\n",
    "# DenseNet-BC with dataset augmentation\n",
    "# Growth rate   | Depth |  Accuracy (paper)| Accuracy (this)      |\n",
    "# 12            | 100   |  95.49%          | 93.74%               |\n",
    "# 24            | 250   |  96.38%          | requires big mem GPU |\n",
    "# 40            | 190   |  96.54%          | requires big mem GPU |\n",
    "growth_rate = 12\n",
    "depth = 100\n",
    "num_bottleneck_layers = (depth - 4) // (2 * num_dense_blocks)\n",
    "\n",
    "num_filters_bef_dense_block = 2 * growth_rate\n",
    "compression_factor = 0.5\n",
    "\n",
    "# load the CIFAR10 data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# input image dimensions\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# mormalize data\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "# convert class vectors to binary class matrices.\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "# start model definition\n",
    "# densenet CNNs (composite function) are made of BN-ReLU-Conv2D\n",
    "inputs = Input(shape=input_shape)\n",
    "x = BatchNormalization()(inputs)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(num_filters_bef_dense_block,\n",
    "           kernel_size=3,\n",
    "           padding='same',\n",
    "           kernel_initializer='he_normal')(x)\n",
    "x = concatenate([inputs, x])\n",
    "\n",
    "# stack of dense blocks bridged by transition layers\n",
    "for i in range(num_dense_blocks):\n",
    "    # a dense block is a stack of bottleneck layers\n",
    "    for j in range(num_bottleneck_layers):\n",
    "        y = BatchNormalization()(x)\n",
    "        y = Activation('relu')(y)\n",
    "        y = Conv2D(4 * growth_rate,\n",
    "                   kernel_size=1,\n",
    "                   padding='same',\n",
    "                   kernel_initializer='he_normal')(y)\n",
    "        if not data_augmentation:\n",
    "            y = Dropout(0.2)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Activation('relu')(y)\n",
    "        y = Conv2D(growth_rate,\n",
    "                   kernel_size=3,\n",
    "                   padding='same',\n",
    "                   kernel_initializer='he_normal')(y)\n",
    "        if not data_augmentation:\n",
    "            y = Dropout(0.2)(y)\n",
    "        x = concatenate([x, y])\n",
    "\n",
    "    # no transition layer after the last dense block\n",
    "    if i == num_dense_blocks - 1:\n",
    "        continue\n",
    "\n",
    "    # transition layer compresses num of feature maps and reduces the size by 2\n",
    "    num_filters_bef_dense_block += num_bottleneck_layers * growth_rate\n",
    "    num_filters_bef_dense_block = int(num_filters_bef_dense_block * compression_factor)\n",
    "    y = BatchNormalization()(x)\n",
    "    y = Conv2D(num_filters_bef_dense_block,\n",
    "               kernel_size=1,\n",
    "               padding='same',\n",
    "               kernel_initializer='he_normal')(y)\n",
    "    if not data_augmentation:\n",
    "        y = Dropout(0.2)(y)\n",
    "    x = AveragePooling2D()(y)\n",
    "\n",
    "\n",
    "# add classifier on top\n",
    "# after average pooling, size of feature map is 1 x 1\n",
    "x = AveragePooling2D(pool_size=8)(x)\n",
    "y = Flatten()(x)\n",
    "outputs = Dense(num_classes,\n",
    "                kernel_initializer='he_normal',\n",
    "                activation='softmax')(y)\n",
    "\n",
    "# instantiate and compile model\n",
    "# orig paper uses SGD but RMSprop works better for DenseNet\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(1e-3),\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "# enable this if pydot can be installed\n",
    "# pip install pydot\n",
    "#plot_model(model, to_file=\"cifar10-densenet.png\", show_shapes=True)\n",
    "model.save('my_model.h5')\n",
    "# prepare model model saving directory\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'cifar10_densenet_model.{epoch:02d}.h5'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "model.save(model_name)\n",
    "\n",
    "# prepare callbacks for model saving and for learning rate reducer\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
    "\n",
    "# run training, with or without data augmentation\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,\n",
    "              callbacks=callbacks)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # preprocessing  and realtime data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (deg 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally\n",
    "        height_shift_range=0.1,  # randomly shift images vertically\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied)\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    steps_per_epoch = math.ceil(len(x_train) / batch_size)\n",
    "    # fit the model on the batches generated by datagen.flow().\n",
    "    model.fit(x=datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "              verbose=1,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              steps_per_epoch=steps_per_epoch,\n",
    "              callbacks=callbacks)\n",
    "\n",
    "\n",
    "    # fit the model on the batches generated by datagen.flow()\n",
    "    #model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "    ##                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "    #                    validation_data=(x_test, y_test),\n",
    "    #                    epochs=epochs, verbose=1,\n",
    "    #                    callbacks=callbacks)\n",
    "\n",
    "# score trained model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc681a220b04d10bf35e74116330022e2509b73a3f531ad28d5c26808ac7ca82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
